---
layout: post
title: Big-O Notation
date: 2024-02-04
tags: dsa
toc: true
comments: true
---

# Concept

In Computer Science, it is very interesting to analyze the cost of an algorithm for a large size of input.

[Big O notation](https://en.wikipedia.org/wiki/Big_O_notation) is one of the most fundamental tools
to measure the cost of an algorithm.

## Big-O Notation

Notice that computer scientists are concerned with how a given function grows when a very large input is given.
So, a small size of input or constant factors are not considered when analyzing the cost of the function.

Big-O is defined as the asymptotic upper bound of a function.

Let $f(x)$ be the function we want to measure, and $g(x)$ be another function that only contains the dominant term of $f(x)$.
Here, $g(x)$ is always less than $f(x)$.

In a formal definition of Big-O,
- $f(x) \in O(g(x))$ if there exists positive constants $c$ and $x_0$ such that $f(x) \leq c*g(x)$ for all $x \geq x_0$.

<div class="img_row">
    <img class="col square" src="{{ site.baseurl }}/assets/img/bigo.png">
</div>

This is pronounced as `Order of ~` or `Big O of ~`.

## Big-O Properties

If the function $f$ can be written as a finite sum of other functions, then the fastest growing one determines the
order of $f(x)$.

$$
f(x) = 4logx + 5(logx)^2 + x^2 + 3x^4 \in O(x^4)\ \ \ \ \ as\ x \rightarrow \infty
$$

In other words, it disregards coefficient, constant, and lower-order terms of the polynomial.

Note that, $log(x^c) = c*log(x) \rightarrow O(log(x^c)) = O(logx)$.

For multiplication by a constant or a function,

<ul>
  <li>$f*O(g) = O(f*g)$</li>
  <li>$O(|k|*g) = O(g)$ where $k$ is constant</li>
</ul>

For multiple functions, when $f_1 \in O(g_1)$ and $f_2 \in O(g_2)$,

<ul>
  <li>$f_1f_2 \in O(g_1g_2)$</li>
  <li>$f_1 + f_2 \in O(max(g_1, g_2))$</li>
</ul>

For multiple variables,

<ul>
  <li>$f(x,y) = 4x^2 + 5y^3 \in O(x^2 + y^3)$</li>
</ul>

## Complexity

Basically, the complexity of an algorithm is determined by the dominant time-consuming section of the algorithm.

Let's apply Big-O properties to some algorihtms.

{% highlight python linenos %}
N = 100 # Input Size
nums = [[0]*N]*N

# O(N^2)
for i in range(N):
    for j in range(N):
        print(nums[i][j])

# O(N^2 / 2) -> O(N^2) : 정확하게는 1~N까지의 총합을 구하는 시그마 공식과 동일하나 계수는 포함X
for i in range(N):
    for j in range(i+1, N):
        print(nums[i][j])

# O(N * (N^3 / 2)) = O(N^4)
for i in range(N):
    j = 0
    while j < N*N*N:
        j = j + 2
        print(i, j)

# O(N+C) = O(N)
{% endhighlight %}


# Application

If a function body contains iteration, the most dominant iteration defines the cost of the function as shown in the following examples.

{% highlight python linenos %}
""" Constant Time : O(1) """
def add(x: int, y: int) -> int:
    # There is no iteration.
    return x + y


""" Logarithmic Time : O(log(n)) """
def binary_search(arr: list[int], target: int) -> bool:
    lo, hi = 0, len(arr)
    while lo+1 < hi:
        mid = (lo + hi) >> 1
        # Explore the half of the current input size in each iteration.
        if arr[mid] < target:
            lo = mid
        elif arr[mid] > target:
            hi = mid
        else:
            return mid
    return -1


""" Square Root Time : O(sqrt(n)) """
def find_prim_factors(n: int) -> set:
    ret = set()
    i, x = 2, n
    while i*i <= n:
        if x % i == 0:
            ret.add(i)
            x //= i
            continue
        i += 1
    if x > 1:
        ret.add(x)
    return ret


""" Linear Time : O(n) """
def sum_array(arr: list[int]) -> int:
    # Iterate over elements exactly once.
    _sum = 0
    for elem in arr:
        _sum += elem
    return _sum


""" Squared Time : O(n^2) """
def add_matrix(m1: list[list], m2: list[list]) -> list[list]:
    # Nested loop is required.
    n = len(m1)
    m3 = [[0]*n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            m3[i][j] = m1[i][j] + m2[i][j]
    return m3


""" Cubed Time : O(n^3) """
def multiply_matrix(m1: list[list], m2: list[list]) -> list[list]:
    # Something like finding triplets requires 3-nested loops.
    n = len(m1)
    m3 = [[0]*n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            for k in range(n):
                m3[i][j] += m1[i][k] * m2[k][j]
    return m3
{% endhighlight %}

If a function body contains recursion calls, how many times it invokes itself and when it ends up will affect the cost.

{% highlight python linenos %}
""" Logarithmic Time : O(log(n)) """
def binary_search(arr: list[int], lo: int, hi: int, target: int) -> int:
    if lo > hi:
        return -1
    mid = (lo + hi) >> 1
    if arr[mid] > target:
        return binary_search(arr, lo, mid-1, target)
    if arr[mid] < target:
        return binary_search(arr, mid+1, hi, target)
    return mid


""" Linear Time : O(n) """
def calc_factorial(n: int) -> int:
    if n == 1:
        return n
    return n * calc_factorial(n-1)


""" Linear Logarithmic Time : O(n*log(n)) """
def merge_sort(elems: list, begin: int, end: int) -> None:
    # Base case: do not have to sort an array of length 1. 
    if begin == end:
        return

    # Recursive case: sort both the left and right half respectively in place.
    mid = (begin + end) >> 1
    merge_sort(elems, begin, mid)
    merge_sort(elems, mid+1, end)

    # Merge two sorted parts into one.
    temp = [-1] * (end-begin+1)
    l, r, i = begin, mid+1, 0
    while l <= mid and r <= end:
        if elems[l] <= elems[r]:
            temp[i] = elems[l]
            l += 1
        else:
            temp[i] = elems[r]
            r += 1
        i += 1

    # Move the rest of the left half part into a temporary array.
    while l <= mid:
        temp[i] = elems[l]
        l += 1
        i += 1

    # Move the rest of the right half part into a temporary array.
    while r <= end:
        temp[i] = elems[r]
        r += 1
        i += 1

    # Copy it into the original array.
    for i in range(begin, end+1):
        elems[i] = temp[i-begin]


""" Exponential Time : O(c^n) """
def recurse(arr: list[int], pos: int, c: int) -> None:
    # Base case: it reaches the very end of an array.
    if pos == len(arr):
        return

    # Decision tree with the height of N has c branches in every invocation.
    for i in range(pos, pos+c):
        recurse(arr, i, c)


""" Factorial Time : O(n!) """
def permutation(arr: list[int], used: int = 0, temp: list[int] = []) -> None:
    # Base case: all elements are used.
    if used == (1 << len(arr))-1:
        print(temp)
        return
    # Explore the next number by choosing an unused element in this recursion.
    for i, elem in enumerate(arr):
        if used & (1 << i):
            continue
        temp.append(elem)
        permutation(arr, used | (1 << i), temp)
        temp.pop()
{% endhighlight %}

<hr>

Similarly, there are different methods to mearsure the cost of an algorithm.

| Type | Description |
|------|-------------|
| **Big O (O())** | the **upper bound** of the complexity |
| **Omega (Ω())** | the **lower bound** of the complexity |
| **Theta (Θ())** | the **exact bound** of the complexity |
| **Little O (o())** | the **upper bound excluding the exact bound** |

<hr>

For common data structure operations

https://www.bigocheatsheet.com/
