---
layout: post
title: Big-O Notation
date: 2024-02-04
tags: dsa
toc: true
comments: true
---

# Concept

In Computer Science, it is very interesting to analyze the cost of an algorithm for a large size of input.

[Big O notation](https://en.wikipedia.org/wiki/Big_O_notation) is one of the most fundamental tools to represent the cost of an algorithm.

## Big-O Notation

Notice that computer scientists are concerned with how a given function grows when a very large input is given.
So, a small size of input or constant factors are not considered when analyzing the cost of the function.

Big-O is defined as the asymptotic upper bound of a function.

Let $f(x)$ be the function we want to measure, and $g(x)$ be another function that only contains the dominant term of $f(x)$.
Here, $g(x)$ is always less than $f(x)$.

In a formal definition of Big-O,
- $f(x) \in O(g(x))$ if there exists positive constants $c$ and $x_0$ such that $f(x) \leq c*g(x)$ for all $x \geq x_0$.

<div class="img_row">
    <img class="col square" src="{{ site.baseurl }}/assets/img/bigo.png">
</div>

This is pronounced as `Order of ~` or `Big O of ~`.

## Big-O Properties

If the function $f$ can be written as a finite sum of other functions, then the fastest growing one determines the
order of $f(x)$.

$$
f(x) = 4logx + 5(logx)^2 + x^2 + 3x^4 \in O(x^4)\ \ \ \ \ as\ x \rightarrow \infty
$$

In other words, it disregards coefficient, constant, and lower-order terms of the polynomial.

Note that, $log(x^c) = c*log(x) \rightarrow O(log(x^c)) = O(logx)$.

For multiplication by a constant or a function,

<ul>
  <li>$f*O(g) = O(f*g)$</li>
  <li>$O(|k|*g) = O(g)$ where $k$ is constant</li>
</ul>

For multiple functions, when $f_1 \in O(g_1)$ and $f_2 \in O(g_2)$,

<ul>
  <li>$f_1f_2 \in O(g_1g_2)$</li>
  <li>$f_1 + f_2 \in O(max(g_1, g_2))$</li>
</ul>

For multiple variables,

<ul>
  <li>$f(x,y) = 4x^2 + 5y^3 \in O(x^2 + y^3)$</li>
</ul>

## How to measure

Basically, the cost of an algorithm is determined by the dominant part of the algorithm.

If a function body contains iteration, the most dominant iteration defines the cost of the function as like below.

{% highlight python linenos %}
""" Constant Time : O(1) """
def add(x: int, y: int) -> int:
    return x + y


""" Linear Time : O(n) """
def sum(n: int) -> int:
    _sum = 0
    for i in range(n):
        _sum += i
    return _sum


""" Squared Time : O(n^2) """
def add_matrix(m1: list[list], m2: list[list]) -> list[list]:
    n = len(m1)
    m3 = [[0]*n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            m3[i][j] = m1[i][j] + m2[i][j]
    return m3


""" Cubed Time : O(n^3) """
{% endhighlight %}

If a function body contains recursion calls, how many times it invokes itself and when it ends up will affect the cost.

{% highlight python linenos %}
""" Linear Time : O(n) """


""" Exponential Time : O(n) """
{% endhighlight %}

<hr>

Similarly, there are different methods to mearsure the cost of an algorithm.

| Type | Description |
|------|-------------|
| **Big O (O())** | the **upper bound** of the complexity |
| **Omega (Ω())** | the **lower bound** of the complexity |
| **Theta (Θ())** | the **exact bound** of the complexity |
| **Little O (o())** | the **upper bound excluding the exact bound** |

# Application

For common data structure operations

https://www.bigocheatsheet.com/
